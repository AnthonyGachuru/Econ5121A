{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asymptotics\n",
    "===========\n",
    "\n",
    "Asymptotic theory is concerned about the behavior of statistics when the\n",
    "sample size is arbitrarily large. It is a useful approximation technique\n",
    "to simplify complicated finite-sample analysis.\n",
    "\n",
    "Modes of Convergence\n",
    "--------------------\n",
    "\n",
    "Convergence of a deterministic sequence means that for any\n",
    "$\\varepsilon>0$, there exists an $N\\left(\\varepsilon\\right)$ such that\n",
    "for all $n>N\\left(\\varepsilon\\right)$, we have\n",
    "$\\left|z_{n}-z\\right|<\\varepsilon$. We say $z$ is the limit of $z_{n}$,\n",
    "and write as $z_{n}\\to z$.\n",
    "\n",
    "In contrast to the convergence of a deterministic sequence, we are\n",
    "interested in the convergence of random variables. Since a random\n",
    "variable is “random”, we must define clearly what “convergence” means.\n",
    "Several modes of convergence are often encountered.\n",
    "\n",
    "-   Convergence almost surely<span>\\*</span>\n",
    "\n",
    "-   Convergence in probability:\n",
    "    $\\lim_{n\\to\\infty}P\\left(\\omega:\\left|Z_{n}\\left(\\omega\\right)-z\\right|<\\varepsilon\\right)=1$\n",
    "    for any $\\varepsilon>0$.\n",
    "\n",
    "-   Squared-mean convergence:\n",
    "    $\\lim_{n\\to\\infty}E\\left[\\left(z_{n}-z\\right)^{2}\\right]=0.$\n",
    "\n",
    "$z_{n}$ is a binary random variable: $z_{n}=\\sqrt{n}$ with probability\n",
    "$1/n$, and $z_{n}=0$ with probability $1-1/n$. Then\n",
    "$z_{n}\\stackrel{p}{\\to}0$ but $z_{n}\\stackrel{m.s.}{\\nrightarrow}0$.\n",
    "\n",
    "Another example (to be formalized): Gambling. Each person contribute one\n",
    "dollar, and only one person will win all the sum. As the number of\n",
    "people goes larger, the average gain is still one dollar, but the limit\n",
    "in probability becomes zero.\n",
    "\n",
    "Convergence in probability does not count what happens on a subset in\n",
    "the sample space of small probability. Squared-mean convergence deals\n",
    "with the average over the entire probability space. If a random variable\n",
    "can take a wild value, even with small probability, it may blow away the\n",
    "squared-mean convergence. On the contrary, such irregularity does not\n",
    "undermine convergence in probability.\n",
    "\n",
    "-   Convergence in distribution: $x_{n}\\stackrel{d}{\\to}x$ if\n",
    "    $F\\left(x_{n}\\right)\\to F\\left(x\\right)$ for each $x$ on which\n",
    "    $F\\left(x\\right)$ is continuous.\n",
    "\n",
    "Convergence in distribution is about *pointwise* convergence of CDF, not\n",
    "the random variables themselves.\n",
    "\n",
    "Let $x\\sim N\\left(0,1\\right)$. If $z_{n}=x+1/n$, then\n",
    "$z_{n}\\stackrel{p}{\\to}x$ and of course $z_{n}\\stackrel{d}{\\to}x$.\n",
    "However, if $z_{n}=-x+1/n$, or $z_{n}=y+1/n$ where\n",
    "$y\\sim N\\left(0,1\\right)$ is independent of $x$, then\n",
    "$z_{n}\\stackrel{d}{\\to}x$ but $z_{n}\\stackrel{p}{\\nrightarrow}x$.\n",
    "\n",
    "*Cramér-Wold device* handles convergence in distribution for random\n",
    "vectors? We say a sequence of $K$-dimensional random vectors\n",
    "$\\left(X_{n}\\right)$ converge in distribution to $X$ if we have\n",
    "$\\lambda'X_{n}\\stackrel{d}{\\to}\\lambda'X$ for any\n",
    "$\\lambda\\in\\mathbb{R}^{K}$ with $\\lambda'\\lambda=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Law of Large Numbers[^1]\n",
    "------------------------\n",
    "\n",
    "(Weak) law of large numbers (LLN) is a collection of statements about\n",
    "convergence in probability of the sample average to its population\n",
    "counterpart. The basic form of LLN is:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}z_{i}-E\\left[\\frac{1}{n}\\sum_{i=1}^{n}z_{i}\\right]\\stackrel{p}{\\to}0$$\n",
    "as $n\\to\\infty$. Various versions of LLN work under different\n",
    "assumptions about the distributions and dependence of the random\n",
    "variables.\n",
    "\n",
    "-   Chebyshev LLN: if $\\left(z_{1},\\ldots,z_{n}\\right)$ is a sample of\n",
    "    i.i.d. observations, $E\\left[z_{1}\\right]=\\mu$ , and\n",
    "    $\\sigma^{2}=\\mathrm{var}\\left[x_{1}\\right]<\\infty$ exists, then\n",
    "    $\\frac{1}{n}\\sum_{i=1}^{n}z_{i}-\\mu\\stackrel{p}{\\to}0.$\n",
    "\n",
    "Chebyshev LLN utilizes\n",
    "\n",
    "-   *Chebyshev inequality*: for any random variable $x$ , we have\n",
    "    $P\\left(\\left|x\\right|>\\varepsilon\\right)\\leq E\\left[x^{2}\\right]/\\varepsilon^{2}$\n",
    "    for any $\\varepsilon>0$, if $E\\left[x^{2}\\right]$ exists.\n",
    "\n",
    "Chebyshev inequality is a special case of\n",
    "\n",
    "-   *Markov inequality*:\n",
    "    $P\\left(\\left|x\\right|>\\varepsilon\\right)\\leq E\\left[\\left|x\\right|^{r}\\right]/\\varepsilon^{r}$\n",
    "    for $r\\geq1$ and any $\\varepsilon>0$, if\n",
    "    $E\\left[\\left|x\\right|^{r}\\right]$ exists.\n",
    "\n",
    "It is easy to verify Markov inequality. $$\\begin{aligned}\n",
    "E\\left[\\left|x\\right|^{r}\\right] & =\\int_{\\left|x\\right|>\\varepsilon}\\left|x\\right|^{r}dF_{X}+\\int_{\\left|x\\right|\\leq\\varepsilon}\\left|x\\right|^{r}dF_{X}\\\\\n",
    " & \\geq\\int_{\\left|x\\right|>\\varepsilon}\\left|x\\right|^{r}dF_{X}\\geq\\varepsilon^{r}\\int_{\\left|x\\right|>\\varepsilon}dF_{X}=\\varepsilon^{r}P\\left(\\left|x\\right|>\\varepsilon\\right).\\end{aligned}$$\n",
    "\n",
    "Consider a partial sum $S_{n}=\\sum_{i=1}^{n}x_{i}$, where\n",
    "$\\mu_{i}=E\\left[x_{i}\\right]$ and\n",
    "$\\sigma_{i}^{2}=\\mathrm{var}\\left[x_{i}\\right]$. We apply the Chebyshev\n",
    "inequality to the sample mean\n",
    "$\\overline{x}-\\bar{\\mu}=n^{-1}\\left(S_{n}-E\\left[S_{n}\\right]\\right)$.\n",
    "$$\\begin{aligned}\n",
    "P\\left(\\left|\\bar{x}-\\bar{\\mu}\\right|\\geq\\varepsilon\\right) & =P\\left(\\left|S_{n}-E\\left[S_{n}\\right]\\right|\\geq n\\varepsilon\\right)\\\\\n",
    " & \\leq\\left(n\\varepsilon\\right)^{-2}E\\left[\\sum_{i=1}^{n}\\left(x_{i}-\\mu_{i}\\right)^{2}\\right]\\\\\n",
    " & =\\left(n\\varepsilon\\right)^{-2}\\mathrm{var}\\left(\\sum_{i=1}^{n}x_{i}\\right)\\\\\n",
    " & =\\left(n\\varepsilon\\right)^{-2}\\left[\\sum_{i=1}^{n}\\mathrm{var}\\left(x_{i}\\right)+\\sum_{i=1}^{n}\\sum_{j\\neq i}\\mathrm{cov}\\left(x_{i},x_{j}\\right)\\right].\\end{aligned}$$\n",
    "\n",
    "From the above derivation, convergence in probability holds as long as\n",
    "the right-hand side shrinks to 0 as $n\\to\\infty$. Actually, the\n",
    "convergence can be maintained under much more general conditions than\n",
    "just under the i.i.d. assumption. The random variables in the sample do\n",
    "not have to be identically distributed, and they do not have to be\n",
    "independent either.\n",
    "\n",
    "Another useful LLN is *Kolmogorov LLN*. Since its derivation requires\n",
    "advanced knowledge of mathematics, we state the result without proof.\n",
    "\n",
    "-   Kolmogorov LLN: if $\\left(z_{1},\\ldots,z_{n}\\right)$ is a sample of\n",
    "    i.i.d. observations and $E\\left[z_{1}\\right]=\\mu$ exists, then\n",
    "    $\\frac{1}{n}\\sum_{i=1}^{n}z_{i}-\\mu\\stackrel{p}{\\to}0.$\n",
    "\n",
    "Compared to Chebyshev LLN, Kolmogorov LLN only requires the existence of\n",
    "the population mean, but not any higher moment. On the other hand,\n",
    "i.i.d. is essential for Kolmogorov LLN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Central Limit Theorem\n",
    "---------------------\n",
    "\n",
    "The central limit theorem (CLT) is a collect of probability results\n",
    "about the convergence in distribution to a normally distributed random\n",
    "variable. The basic form of the CLT is: for a sample\n",
    "$\\left(z_{1},\\ldots,z_{n}\\right)$ of *zero-mean* random variables,\n",
    "$$\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}z_{i}\\stackrel{d}{\\to}N\\left(0,\\sigma^{2}\\right).\\label{eq:clt}$$\n",
    "Various versions of CLT work under different assumptions about the\n",
    "random variables.\n",
    "\n",
    "*Lindeberg-Levy CLT* is the simplest CLT.\n",
    "\n",
    "-   If the sample is i.i.d., $E\\left[x_{1}\\right]=0$ and\n",
    "    $\\mathrm{var}\\left[x_{1}^{2}\\right]=\\sigma^{2}<\\infty$,\n",
    "    then (\\[eq:clt\\]) holds.\n",
    "\n",
    "Lindeberg-Levy CLT is easy to verify by the characteristic function. For\n",
    "any random variable $x$, the function\n",
    "$\\varphi_{x}\\left(t\\right)=E\\left[\\exp\\left(ixt\\right)\\right]$ is called\n",
    "its *characteristic function*. The characteristic function fully\n",
    "describes a distribution, just like PDF or CDF. For example, the\n",
    "characteristic function of $N\\left(\\mu,\\sigma^{2}\\right)$ is\n",
    "$\\exp\\left(it\\mu-\\frac{1}{2}\\sigma^{2}t^{2}\\right)$.\n",
    "\n",
    "If $E\\left[\\left|x\\right|^{k}\\right]<\\infty$ for a positive integer $k$,\n",
    "then\n",
    "$$\\varphi_{X}\\left(t\\right)=1+itE\\left[X\\right]+\\frac{\\left(it\\right)^{2}}{2}E\\left[X^{2}\\right]+\\ldots\\frac{\\left(it\\right)^{k}}{k!}E\\left[X^{k}\\right]+o\\left(t^{k}\\right).$$\n",
    "Under the assumption of Lindeberg-Levy CLT,\n",
    "$$\\varphi_{X_{i}/\\sqrt{n}}\\left(t\\right)=1-\\frac{t^{2}}{2n}\\sigma^{2}+o\\left(\\frac{t^{2}}{n}\\right)$$\n",
    "for all $i$, and by independence we have $$\\begin{aligned}\n",
    "\\varphi_{\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}x_{i}}\\left(t\\right) & =\\prod_{i=1}^{n}\\varphi_{x_{i}/\\sqrt{n}}\\left(t\\right)=\\left(1+i\\cdot0-\\frac{t^{2}}{2n}\\sigma^{2}+o\\left(\\frac{t^{2}}{n}\\right)\\right)^{n}\\\\\n",
    " & \\to\\exp\\left(-\\frac{\\sigma^{2}}{2}t^{2}\\right),\\end{aligned}$$ where\n",
    "the limit is exactly the characteristic function of\n",
    "$N\\left(0,\\sigma^{2}\\right)$.\n",
    "\n",
    "-   Lindeberg-Feller CLT: i.n.i.d., and *Lindeberg condition*: for any\n",
    "    fixed $\\varepsilon>0$,\n",
    "    $$\\frac{1}{s_{n}^{2}}\\sum_{i=1}^{n}\\int_{\\left|x_{i}\\right|>\\varepsilon s_{n}}x_{i}^{2}dPx_{i}\\to0$$\n",
    "    where $s_{n}=\\left(\\sum_{i=1}^{n}\\sigma_{i}^{2}\\right)^{1/2}$.\n",
    "\n",
    "-   Lyapunov CLT: i.n.i.d, finite $E\\left[\\left|x\\right|^{3}\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools for Transformations\n",
    "-------------------------\n",
    "\n",
    "The original forms of LLN or CLT only deal with sample means. However,\n",
    "most of the econometric estimators of interest are functions of sample\n",
    "means. Therefore, we need tools to handle transformations.\n",
    "\n",
    "-   Small op: $x_{n}=o_{p}\\left(r_{n}\\right)$ if\n",
    "    $x_{n}/r_{n}\\stackrel{p}{\\to}0$.\n",
    "\n",
    "-   Big Op: $x_{n}=O_{p}\\left(r_{n}\\right)$ if for any $\\varepsilon>0$,\n",
    "    there exists a $c>0$ such that\n",
    "    $P\\left(\\left|x_{n}\\right|/r_{n}>c\\right)<\\varepsilon$.\n",
    "\n",
    "-   Continuous mapping theorem 1: If $x_{n}\\stackrel{p}{\\to}a$ and\n",
    "    $f\\left(\\cdot\\right)$ is continuous at $a$, then\n",
    "    $f\\left(x_{n}\\right)\\stackrel{p}{\\to}f\\left(a\\right)$.\n",
    "\n",
    "-   Continuous mapping theorem 2: If $x_{n}\\stackrel{d}{\\to}x$ and\n",
    "    $f\\left(\\cdot\\right)$ is continuous almost surely on the support of\n",
    "    $x$, then $f\\left(x_{n}\\right)\\stackrel{d}{\\to}f\\left(x\\right)$.\n",
    "\n",
    "-   Slutsky’s Theorem: If $x_{n}\\stackrel{d}{\\to}x$ and\n",
    "    $y_{n}\\stackrel{p}{\\to}a$, then\n",
    "\n",
    "    -   $x_{n}+y_{n}\\stackrel{d}{\\to}x+a$\n",
    "\n",
    "    -   $x_{n}y_{n}\\stackrel{d}{\\to}ax$\n",
    "\n",
    "    -   $x_{n}/y_{n}\\stackrel{d}{\\to}x/a$ if $a\\neq0$.\n",
    "\n",
    "-   Delta method: if\n",
    "    $\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\stackrel{d}{\\to}N\\left(0,\\Omega\\right)$,\n",
    "    and $f\\left(\\cdot\\right)$ is continuously differentiable at\n",
    "    $\\theta_{0}$, then\n",
    "    $$\\sqrt{n}\\left(f\\left(\\widehat{\\theta}\\right)-f\\left(\\theta_{0}\\right)\\right)\\stackrel{d}{\\to}N\\left(0,\\frac{\\partial f}{\\partial\\theta'}\\left(\\theta_{0}\\right)\\Omega\\left(\\frac{\\partial f}{\\partial\\theta}\\left(\\theta_{0}\\right)\\right)'\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asymptotic Properties of OLS\n",
    "============================\n",
    "\n",
    "We apply large sample theory to study the OLS estimator\n",
    "$\\widehat{\\beta}=\\left(X'X\\right)^{-1}X'Y.$\n",
    "\n",
    "Consistency\n",
    "-----------\n",
    "\n",
    "We say $\\widehat{\\beta}$ is *consistent* if\n",
    "$\\widehat{\\beta}\\stackrel{p}{\\to}\\beta$ as $n\\to\\infty$. To verify\n",
    "consistency, we write\n",
    "$$\\widehat{\\beta}-\\beta=\\left(X'X\\right)^{-1}X'e=\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}'\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}e_{i}.\\label{eq:ols_d}$$\n",
    "The first term\n",
    "$$\\widehat{Q}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}'\\stackrel{p}{\\to}Q=E\\left[x_{i}x_{i}'\\right].$$\n",
    "and the second term\n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}x_{i}e_{i}\\stackrel{p}{\\to}0.$$ No matter\n",
    "whether $\\left(y_{i},x_{i}\\right)_{i=1}^{n}$ is an i.i.d., i.n.i.d., or\n",
    "dependent sample, as long as the convergence in probability holds for\n",
    "the above two expressions, we have\n",
    "$\\widehat{\\beta}-\\beta\\stackrel{p}{\\to}Q^{-1}0=0$ by the continuous\n",
    "mapping theorem. In other words, $\\widehat{\\beta}$ is a consistent\n",
    "estimator of $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asymptotic Normality\n",
    "--------------------\n",
    "\n",
    "In finite sample, $\\widehat{\\beta}$ is a random variable. We have shown\n",
    "the distribution of $\\widehat{\\beta}$ under normality in the previous\n",
    "lecture. Without the restrictive normality assumption, how can we\n",
    "characterize the randomness of the OLS estimator? If we multiply\n",
    "$\\sqrt{n}$ on both sides of (\\[eq:ols\\_d\\]), we have\n",
    "$$\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)=\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}'\\right)^{-1}\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}x_{i}e_{i}.$$\n",
    "Since $E\\left[x_{i}e_{i}\\right]=0$, we apply a CLT to obtain\n",
    "$$n^{-1/2}\\sum_{i=1}^{n}x_{i}e_{i}\\stackrel{d}{\\to}N\\left(0,\\Sigma\\right)$$\n",
    "where $\\Sigma=E\\left[x_{i}x_{i}'e_{i}^{2}\\right]$. By the continuous\n",
    "mapping theorem,\n",
    "$$\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\to}Q^{-1}\\times N\\left(0,\\Sigma\\right)\\sim N\\left(0,\\Omega\\right)$$\n",
    "where $\\Omega=Q^{-1}\\Sigma Q^{-1}$ is called the *asymptotic variance*.\n",
    "This is the *asymptotic normality* of the OLS estimator.\n",
    "\n",
    "Up to now we have derived the asymptotic distribution of\n",
    "$\\widehat{\\beta}$. However, to make it feasible, we still have to\n",
    "estimator the asymptotic variance $\\Omega$. If $\\widehat{\\Sigma}$ is a\n",
    "consistent estimator of $\\Sigma$, then\n",
    "$\\widehat{\\Omega}=\\widehat{Q}^{-1}\\widehat{\\Sigma}\\widehat{Q}^{-1}$ is a\n",
    "consistent estimator of $\\Omega$. (Of course, there are other ways to\n",
    "estimate the asymptotic variance.) Then a feasible version about the\n",
    "distribution of $\\widehat{\\beta}$ is\n",
    "$$\\widehat{\\Omega}^{-1/2}\\sqrt{n}\\left(\\widehat{\\beta}-\\beta\\right)\\stackrel{d}{\\to}N\\left(0,I_{K}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation of the Variance<span>\\*</span>\n",
    "-----------------------------------------\n",
    "\n",
    "To show the finiteness of the variance,\n",
    "$\\Sigma=E\\left[x_{i}x_{i}'e_{i}^{2}\\right].$ Let $z_{i}=x_{i}e_{i}$, so\n",
    "$\\Sigma=E\\left[z_{i}z_{i}'\\right]$. Because of the Cachy-Schwarz\n",
    "inequality,\n",
    "$$\\left\\Vert \\Sigma\\right\\Vert _{\\infty}=\\max_{k=1,\\ldots,K}E\\left[z_{ik}^{2}\\right].$$\n",
    "For each $k$,\n",
    "$E\\left[z_{ik}^{2}\\right]=E\\left[x_{ik}^{2}e_{i}^{2}\\right]\\leq\\left(E\\left[x_{ik}^{4}\\right]E\\left[e_{i}^{4}\\right]\\right)^{1/2}$.\n",
    "\n",
    "For the estimation of variance, if the error is homoskedastic,\n",
    "$$\\begin{aligned}\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}\\widehat{e}_{i}^{2} & =\\frac{1}{n}\\sum_{i=1}^{n}\\left(e_{i}+x_{i}'\\left(\\widehat{\\beta}-\\beta\\right)\\right)^{2}\\\\\n",
    " & =\\frac{1}{n}\\sum_{i=1}^{n}e_{i}^{2}+\\left(\\frac{2}{n}\\sum_{i=1}^{n}e_{i}x_{i}\\right)'\\left(\\widehat{\\beta}-\\beta\\right)+\\frac{1}{n}\\sum_{i=1}^{n}e_{i}^{2}\\left(\\widehat{\\beta}-\\beta\\right)'x_{i}x_{i}'\\left(\\widehat{\\beta}-\\beta\\right).\\end{aligned}$$\n",
    "The second term\n",
    "$$\\left(\\frac{2}{n}\\sum_{i=1}^{n}e_{i}x_{i}\\right)'\\left(\\widehat{\\beta}-\\beta\\right)=o_{p}\\left(1\\right)o_{p}\\left(1\\right)=o_{p}\\left(1\\right).$$\n",
    "The third term\n",
    "$$\\left(\\widehat{\\beta}-\\beta\\right)\\left(\\frac{1}{n}\\sum_{i=1}^{n}e_{i}^{2}x_{i}x'_{i}\\right)\\left(\\widehat{\\beta}-\\beta\\right)=o_{p}\\left(1\\right)O_{p}\\left(1\\right)o_{p}\\left(1\\right)=o_{p}\\left(1\\right).$$\n",
    "As\n",
    "$\\frac{1}{n}\\sum_{i=1}^{n}\\widehat{e}_{i}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\widehat{e}_{i}^{2}+o_{p}\\left(1\\right)$\n",
    "and\n",
    "$\\frac{1}{n}\\sum_{i=1}^{n}e_{i}^{2}=\\sigma_{e}^{2}+o_{p}\\left(1\\right)$,\n",
    "we have\n",
    "$\\frac{1}{n}\\sum_{i=1}^{n}\\widehat{e}_{i}^{2}=\\sigma_{e}^{2}+o_{p}\\left(1\\right)$.\n",
    "In other words,\n",
    "$\\frac{1}{n}\\sum_{i=1}^{n}\\widehat{e}_{i}^{2}\\stackrel{p}{\\to}\\sigma_{e}^{2}$.\n",
    "\n",
    "For general heteroskedasticity, $$\\begin{aligned}\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}\\widehat{e}_{i}^{2} & =\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}'\\left(e_{i}+x_{i}'\\left(\\widehat{\\beta}-\\beta\\right)\\right)^{2}\\\\\n",
    " & =\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}'e_{i}^{2}+\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}e_{i}x_{i}'\\left(\\widehat{\\beta}-\\beta\\right)+\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}'\\left(\\left(\\widehat{\\beta}-\\beta\\right)'x_{i}\\right)^{2}.\\end{aligned}$$\n",
    "The third term is bounded by $$\\begin{aligned}\n",
    " &  & \\mbox{trace}\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}'\\left(\\left(\\widehat{\\beta}-\\beta\\right)'x_{i}\\right)^{2}\\right)\\\\\n",
    " & \\leq & K\\max_{k}\\frac{1}{n}\\sum_{i=1}^{n}x_{ik}^{2}\\left[\\left(\\widehat{\\beta}-\\beta\\right)'x_{i}\\right]^{2}\\\\\n",
    " & \\leq & K\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\max_{k}\\frac{1}{n}\\sum_{i=1}^{n}x_{ik}^{2}\\left\\Vert x_{i}\\right\\Vert _{2}^{2}\\\\\n",
    " & \\leq & K\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert x_{i}\\right\\Vert _{2}^{2}\\left\\Vert x_{i}\\right\\Vert _{2}^{2}\\\\\n",
    " & = & K\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\sum_{k=1}^{K}x_{ik}^{2}\\right)^{2}\\\\\n",
    " & \\leq & K\\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}^{2}K\\sum_{k=1}^{K}\\frac{1}{n}\\sum_{i=1}^{n}x_{ik}^{4}=o_{p}\\left(1\\right)O_{p}\\left(1\\right)=o_{p}\\left(1\\right).\\end{aligned}$$\n",
    "where the third inequality follows by\n",
    "$\\left(a_{1}+\\cdots+a_{K}\\right)^{2}\\leq K\\left(a_{1}^{2}+\\cdots+a_{K}^{2}\\right)$.\n",
    "The second term is bounded by $$\\begin{aligned}\n",
    " &  & \\left|\\frac{1}{n}\\sum_{i=1}^{n}x_{ik}x_{ik'}e_{i}x_{i}'\\left(\\widehat{\\beta}-\\beta\\right)\\right|\\\\\n",
    " & \\leq & \\max_{k}\\left|\\widehat{\\beta}_{k}-\\beta_{k}\\right|K\\max_{k,k',k''}\\left|\\frac{1}{n}\\sum_{i=1}^{n}e_{i}x_{ik}x_{ik'}x_{ik''}\\right|\\\\\n",
    " & \\leq & \\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}\\left(\\frac{1}{n}\\sum_{i=1}^{n}e_{i}^{4}\\right)^{1/4}K\\max_{k,k',k''}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{ik}x_{ik'}x_{ik''}\\right)^{4/3}\\right)^{3/4}\\\\\n",
    " & \\leq & \\left\\Vert \\widehat{\\beta}-\\beta\\right\\Vert _{2}K\\max_{k}\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{ik}^{4}\\right)^{3/4}=o_{p}\\left(1\\right)O_{p}\\left(1\\right)\\end{aligned}$$\n",
    "where the second and the third inequality hold by the Holder’s\n",
    "inequality.\n",
    "\n",
    "[^1]: Though the results in this section hold for convergence almost\n",
    "    surely, for simplicity we state them in terms of convergence in\n",
    "    probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
