{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version:\n",
    "\n",
    "Notation: $y_{i}$ is a scalar, and $x_{i}$ is a $K\\times1$ vector. $Y$\n",
    "is an $n\\times1$ vector, and $X$ is an $n\\times K$ matrix.\n",
    "\n",
    "Algebra of Least Squares\n",
    "========================\n",
    "\n",
    "OLS estimator\n",
    "-------------\n",
    "\n",
    "As we have learned from the linear project model, the parameter $\\beta$\n",
    "$$\\begin{aligned}\n",
    "y_{i} & = & x'_{i}\\beta+e_{i}\\\\\n",
    "E[x_{i}e_{i}] & = & 0\\end{aligned}$$ can be written as\n",
    "$\\beta=\\left(E\\left[x_{i}x_{i}'\\right]\\right)^{-1}E\\left[x_{i}y_{i}\\right].$\n",
    "\n",
    "While population is something imaginary, in reality we possess a sample\n",
    "of $n$ observations. We thus replace the population mean\n",
    "$E\\left[\\cdot\\right]$ by the sample mean, and the resulting estimator is\n",
    "$$\\widehat{\\beta}=\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}'\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}y_{i}=\\left(X'X\\right)^{-1}X'y.$$\n",
    "This is one way to motivate the OLS estimator.\n",
    "\n",
    "Alternatively, we can derive the OLS estimator from minimizing the sum\n",
    "of squared residuals\n",
    "$$Q\\left(\\beta\\right)=\\sum_{i=1}^{n}\\left(y_{i}-x_{i}'\\beta\\right)^{2}=\\left(Y-X\\beta\\right)'\\left(Y-X\\beta\\right).$$\n",
    "By the first-order condition\n",
    "$$\\frac{\\partial}{\\partial\\beta}Q\\left(\\beta\\right)=-2X'\\left(Y-X\\beta\\right),$$\n",
    "the optimality condition gives exactly the same $\\widehat{\\beta}$.\n",
    "Moreover, the second-order condition\n",
    "$$\\frac{\\partial^{2}}{\\partial\\beta\\partial\\beta'}Q\\left(\\beta\\right)=2X'X$$\n",
    "shows that $Q\\left(\\beta\\right)$ is convex in $\\beta$.\n",
    "($Q\\left(\\beta\\right)$ is strictly convex in $\\beta$ if $X'X$ is\n",
    "positive definite.)\n",
    "\n",
    "Here we introduce some definitions and properties in OLS estimation.\n",
    "\n",
    "-   Fitted value: $\\widehat{Y}=X\\widehat{\\beta}$.\n",
    "\n",
    "-   Projector: $P_{X}=X\\left(X'X\\right)^{-1}X$; Annihilator:\n",
    "    $M_{X}=I_{n}-P_{X}$.\n",
    "\n",
    "-   $P_{X}M_{X}=M_{X}P_{X}=0$.\n",
    "\n",
    "-   If $AA=A$, we call it an idempotent matrix. Both $P_{X}$ and $M_{X}$\n",
    "    are idempotent.\n",
    "\n",
    "-   Residual:\n",
    "    $\\widehat{e}=Y-\\widehat{Y}=Y-X\\widehat{\\beta}=M_{X}Y=M_{X}\\left(X\\beta+e\\right)=M_{X}e$.\n",
    "\n",
    "-   $X'\\widehat{e}=XM_{X}e=0$.\n",
    "\n",
    "-   $\\frac{1}{n}\\sum_{i=1}^{n}\\widehat{e}_{i}=0$ if $x_{i}$ contains a\n",
    "    constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goodness of Fit\n",
    "---------------\n",
    "\n",
    "The so-called R-square is the most popular measure of goodness-of-fit in\n",
    "the linear regression. R-square is well defined only when a constant is\n",
    "included in the regressors. Let\n",
    "$M_{\\iota}=I_{n}-\\frac{1}{n}\\iota\\iota'$, where $\\iota$ is an $n\\times1$\n",
    "vector of 1â€™s. $M_{\\iota}$ is the *demeaner*, in the sense that\n",
    "$M_{\\iota}\\left(z_{1},\\ldots,z_{n}\\right)'=\\left(z_{1}-\\overline{z},\\ldots,z_{n}-\\overline{z}\\right)'$,\n",
    "where $\\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n}z_{i}$. For any $X$, we can\n",
    "decompose $Y=P_{X}Y+M_{X}Y=\\widehat{Y}+\\widehat{e}$. The total variation\n",
    "is\n",
    "$$Y'M_{\\iota}Y=\\left(\\widehat{Y}+\\widehat{e}\\right)'M_{\\iota}\\left(\\widehat{Y}+\\widehat{e}\\right)=\\widehat{Y}'M_{\\iota}\\widehat{Y}+2\\widehat{Y}'M_{\\iota}\\widehat{e}+\\widehat{e}'M_{\\iota}\\widehat{e}=\\widehat{Y}'M_{\\iota}\\widehat{Y}+\\widehat{e}'\\widehat{e}$$\n",
    "where the last equality follows by $M_{\\iota}\\widehat{e}=\\widehat{e}$ as\n",
    "$\\frac{1}{n}\\sum_{i=1}^{n}\\widehat{e}_{i}=0$, and\n",
    "$\\widehat{Y}'\\widehat{e}=Y'P_{X}M_{X}e=0$. R-square is defined as\n",
    "$\\widehat{Y}'M_{\\iota}\\widehat{Y}/Y'M_{\\iota}Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frish-Waugh-Lovell Theorem\n",
    "--------------------------\n",
    "\n",
    "This theorem is the sample version of the subvector regression.\n",
    "\n",
    "If $Y=X_{1}\\beta_{1}+X_{2}\\beta_{2}+e$, then\n",
    "$\\widehat{\\beta}_{1}=\\left(X_{1}'M_{X_{2}}X_{1}\\right)^{-1}X_{1}'M_{X_{2}}Y.$\n",
    "\n",
    "Statistical Properties of Least Squares\n",
    "=======================================\n",
    "\n",
    "To talk about the statistical properties in finite sample, we impose the\n",
    "following assumptions.\n",
    "\n",
    "1.  The data $\\left(y_{i},x_{i}\\right)_{i=1}^{n}$ is a random sample\n",
    "    from the same data generating process $y_{i}=x_{i}'\\beta+e_{i}$.\n",
    "\n",
    "2.  $e_{i}|x_{i}\\sim N\\left(0,\\sigma^{2}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Likelihood Estimation[\\*]{}\n",
    "-----------------------------------\n",
    "\n",
    "Under the normality assumption,\n",
    "$y_{i}|x_{i}\\sim N\\left(x_{i}'\\beta,\\gamma\\right)$, where\n",
    "$\\gamma=\\sigma^{2}$. The *conditional* likelihood of observing a sample\n",
    "$\\left(y_{i},x_{i}\\right)_{i=1}^{n}$ is\n",
    "$$\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi\\gamma}}\\exp\\left(-\\frac{1}{2\\gamma}\\left(y_{i}-x_{i}'\\beta\\right)^{2}\\right),$$\n",
    "and the (conditional) log-likelihood function is\n",
    "$$L\\left(\\beta,\\gamma\\right)=-\\frac{n}{2}\\log2\\pi-\\frac{n}{2}\\log\\gamma-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(y_{i}-x_{i}'\\beta\\right)^{2}.$$\n",
    "Therefore, the maximum likelihood estimator (MLE) coincides with the OLS\n",
    "estimator, and\n",
    "$\\widehat{\\gamma}_{\\mathrm{MLE}}=\\widehat{e}'\\widehat{e}/n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finite Sample Distribution\n",
    "--------------------------\n",
    "\n",
    "We can show the finite-sample exact distribution of $\\widehat{\\beta}$.\n",
    "*Finite sample distribution* means that the distribution holds for any\n",
    "$n$; it is in contrast to *asymptotic distribution*, which holds only\n",
    "when $n$ is arbitrarily large.\n",
    "\n",
    "Since\n",
    "$$\\widehat{\\beta}=\\left(X'X\\right)^{-1}X'y=\\left(X'X\\right)^{-1}X'\\left(X'\\beta+e\\right)=\\beta+\\left(X'X\\right)^{-1}X'e,$$\n",
    "we have the estimator\n",
    "$\\widehat{\\beta}|X\\sim N\\left(\\beta,\\sigma^{2}\\left(X'X\\right)^{-1}\\right)$,\n",
    "and\n",
    "$$\\widehat{\\beta}_{k}|X\\sim N\\left(\\beta_{k},\\sigma^{2}\\eta_{k}'\\left(X'X\\right)^{-1}\\eta_{k}\\right)\\sim N\\left(\\beta_{k},\\sigma^{2}\\left(X'X\\right)_{kk}^{-1}\\right),$$\n",
    "where $\\eta_{k}=\\left(1\\left\\{ l=k\\right\\} \\right)_{l=1,\\ldots,K}$ is\n",
    "the selector of the $k$-th element.\n",
    "\n",
    "In reality, $\\sigma^{2}$ is an unknown parameter, and\n",
    "$$s^{2}=\\widehat{e}'\\widehat{e}/\\left(n-K\\right)=e'M_{X}e/\\left(n-K\\right)$$\n",
    "is an unbiased estimator of $\\sigma^{2}$. Consider the $T$-statistic\n",
    "$$T_{k}=\\frac{\\widehat{\\beta}_{k}-\\beta_{k}}{\\sqrt{s^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}}=\\frac{\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)/\\sqrt{\\sigma^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}}{\\sqrt{\\frac{e'}{\\sigma}M_{X}\\frac{e}{\\sigma}/\\left(n-K\\right)}}.$$\n",
    "The numerator follows a standard normal, and the denominator follows\n",
    "$\\frac{1}{n-K}\\chi^{2}\\left(n-K\\right)$. Moreover, the numerator and the\n",
    "denominator are independent. As a result, $T_{k}\\sim t\\left(n-K\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and Variance\n",
    "-----------------\n",
    "\n",
    "Now we relax the normality assumption and statistical independence.\n",
    "Instead, we assume a random sample and $$\\begin{aligned}\n",
    "y_{i} & = & x_{i}'\\beta+e_{i}\\nonumber \\\\\n",
    "E[e_{i}|x_{i}] & = & 0\\label{eq:mean_indep}\\\\\n",
    "E\\left[e_{i}^{2}|x_{i}\\right] & = & \\sigma^{2}.\\label{eq:homo}\\end{aligned}$$\n",
    "(\\[eq:mean\\_indep\\]) is the *mean independence* assumption, and\n",
    "(\\[eq:homo\\]) is the *homoskedasticity* assumption.\n",
    "\n",
    "(Heteroskedasticity) If $e_{i}=x_{i}u_{i}$, where $x_{i}$ is a scalar\n",
    "random variable, $u_{i}$ is independent of $x_{i}$,\n",
    "$E\\left[u_{i}\\right]=0$ and $E\\left[u_{i}^{2}\\right]=\\sigma^{2}$. Then\n",
    "$E\\left[e_{i}|x_{i}\\right]=0$ but\n",
    "$E\\left[e_{i}^{2}|x_{i}\\right]=\\sigma_{i}^{2}x_{i}^{2}$ is a function of\n",
    "$x_{i}$. We say $e_{i}^{2}$ is a heteroskedastic error.\n",
    "\n",
    "These assumptions are about the first and second moment of $e_{i}$\n",
    "conditional on $x_{i}$. Unlike the normality assumption, they do not\n",
    "restrict the entire distribution of $e_{i}$.\n",
    "\n",
    "-   Unbiasedness:\n",
    "    $$E\\left[\\widehat{\\beta}|X\\right]=E\\left[\\left(X'X\\right)^{-1}XY|X\\right]=E\\left[\\left(X'X\\right)^{-1}X\\left(X'\\beta+e\\right)|X\\right]=\\beta.$$\n",
    "    Unbiasedness does not rely on homoskedasticity.\n",
    "\n",
    "-   Variance: $$\\begin{aligned}\n",
    "    \\mathrm{var}\\left(\\widehat{\\beta}|X\\right) & = & E\\left[\\left(\\widehat{\\beta}-E\\widehat{\\beta}\\right)\\left(\\widehat{\\beta}-E\\widehat{\\beta}\\right)'|X\\right]\\\\\n",
    "     & = & E\\left[\\left(\\widehat{\\beta}-\\beta\\right)\\left(\\widehat{\\beta}-\\beta\\right)'|X\\right]\\\\\n",
    "     & = & E\\left[\\left(X'X\\right)^{-1}X'ee'X\\left(X'X\\right)^{-1}|X\\right]\\\\\n",
    "     & = & \\left(X'X\\right)^{-1}X'E\\left[ee'|X\\right]X\\left(X'X\\right)^{-1}\\\\\n",
    "     & = & \\left(X'X\\right)^{-1}X'\\left(\\sigma^{2}I_{n}\\right)X\\left(X'X\\right)^{-1}\\\\\n",
    "     & = & \\sigma^{2}\\left(X'X\\right)^{-1}.\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gauss-Markov Theorem[\\*]{}\n",
    "--------------------------\n",
    "\n",
    "Gauss-Markov theorem justifies the OLS estimator as the efficient\n",
    "estimator among all linear unbiased ones. *Efficient* here means that it\n",
    "enjoys the smallest variance in a family of estimators.\n",
    "\n",
    "There are numerous linearly unbiased estimators. For example,\n",
    "$\\left(Z'X\\right)^{-1}Z'y$ for $z_{i}=x_{i}^{2}$ is unbiased because\n",
    "$E\\left[\\left(Z'X\\right)^{-1}Z'y\\right]=E\\left[\\left(Z'X\\right)^{-1}Z'\\left(X\\beta+e\\right)\\right]=\\beta$.\n",
    "\n",
    "Let $\\tilde{\\beta}=A'y$ be a generic linear estimator, where $A$ is any\n",
    "$n\\times K$ functions of $X$. As\n",
    "$$E\\left[A'y|X\\right]=E\\left[A'\\left(X\\beta+e\\right)|X\\right]=A'X\\beta.$$\n",
    "So the linearity and unbiasedness of $\\tilde{\\beta}$ implies\n",
    "$A'X=I_{n}$. Moreover, the variance\n",
    "$$\\mbox{var}\\left(A'y|X\\right)=E\\left[\\left(A'y-\\beta\\right)\\left(A'y-\\beta\\right)'|X\\right]=E\\left[A'ee'A|X\\right]=\\sigma^{2}A'A.$$\n",
    "Let $C=A-X\\left(X'X\\right)^{-1}.$ $$\\begin{aligned}\n",
    " &  & A'A-\\left(X'X\\right)^{-1}\\\\\n",
    " & = & \\left(C+X\\left(X'X\\right)^{-1}\\right)'\\left(C+X\\left(X'X\\right)^{-1}\\right)-\\left(X'X\\right)^{-1}\\\\\n",
    " & = & C'C+\\left(X'X\\right)^{-1}X'C+C'X\\left(X'X\\right)^{-1}=C'C,\\end{aligned}$$\n",
    "where the last equality follows as\n",
    "$$\\left(X'X\\right)^{-1}X'C=\\left(X'X\\right)^{-1}X'\\left(A-X\\left(X'X\\right)^{-1}\\right)=\\left(X'X\\right)^{-1}-\\left(X'X\\right)^{-1}=0.$$\n",
    "Therefore $A'A-\\left(X'X\\right)^{-1}$ is a positive semi-definite\n",
    "matrix. The variance of any $\\tilde{\\beta}$ is no smaller than the OLS\n",
    "estimator $\\widehat{\\beta}$.\n",
    "\n",
    "Homoskedasticity is a restrictive assumption. Under homoskedasticity,\n",
    "$\\mathrm{var}\\left(\\widehat{\\beta}\\right)=\\sigma^{2}\\left(X'X\\right)^{-1}$.\n",
    "Popular estimator of $\\sigma^{2}$ is the sample mean of the residuals\n",
    "$\\widehat{\\sigma}^{2}=\\frac{1}{n}\\widehat{e}'\\widehat{e}$ or the\n",
    "unbiased one $s^{2}=\\frac{1}{n-K}\\widehat{e}'\\widehat{e}$. Under\n",
    "heteroskedasticity, Gauss-Markov theorem does not apply."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
