#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Lecture Notes}
\rhead{Zhentao Shi}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams-bytype
theorems-ams-extended-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format dvi
\output_sync 1
\output_sync_macro "\synctex=-1"
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\branch abc
\selected 1
\filename_suffix 0
\color #faf0e6
\end_branch
\branch pf of gamma = 0
\selected 0
\filename_suffix 0
\color #faf0e6
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes true
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
The Algebra of Least Squares
\end_layout

\begin_layout Standard
Notation: 
\begin_inset Formula $y$
\end_inset

 is a scalar, and 
\begin_inset Formula $x$
\end_inset

 is a 
\begin_inset Formula $1\times K$
\end_inset

 vector.
 
\begin_inset Formula $Y$
\end_inset

 is an 
\begin_inset Formula $n\times1$
\end_inset

 vector, and 
\begin_inset Formula $X$
\end_inset

 is an 
\begin_inset Formula $n\times K$
\end_inset

 matrix.
\end_layout

\begin_layout Subsection
OLS estimator
\end_layout

\begin_layout Standard
As we have learned from the previous lecture, the parameter 
\begin_inset Formula $\beta$
\end_inset

 in the linear projection model 
\begin_inset Formula 
\begin{eqnarray*}
y & = & x'\beta+e\\
E[xe] & = & 0
\end{eqnarray*}

\end_inset

can be written as 
\begin_inset Formula $\beta=\left(E\left[xx'\right]\right)^{-1}E\left[xy\right].$
\end_inset

 In reality we possess a sample of 
\begin_inset Formula $n$
\end_inset

 observations, not the population.
 We thus replace the population mean 
\begin_inset Formula $E\left[\cdot\right]$
\end_inset

 by the sample mean, and the resulting estimator is
\begin_inset Formula 
\[
\widehat{\beta}=\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}=\left(X'X\right)^{-1}X'y.
\]

\end_inset

This is one way to motivate the OLS estimator.
 
\end_layout

\begin_layout Standard
Alternatively, we can derive the OLS estimator from minimizing the sum of
 squared residuals 
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-x_{i}'\beta\right)^{2},
\]

\end_inset

which is the sample counterpart of the MSE.
 By a similar optimization procedure, we derive exactly the same 
\begin_inset Formula $\widehat{\beta}$
\end_inset

.
\end_layout

\begin_layout Standard
Some definitions.
\end_layout

\begin_layout Itemize
Fitted value: 
\begin_inset Formula $\widehat{Y}=X\widehat{\beta}$
\end_inset

.
\end_layout

\begin_layout Itemize
Residual: 
\begin_inset Formula $\widehat{e}=Y-\widehat{Y}$
\end_inset

.
\end_layout

\begin_layout Itemize
Projector: 
\begin_inset Formula $P_{X}=X\left(X'X\right)^{-1}X$
\end_inset

; Annihilator: 
\begin_inset Formula $M_{X}=I_{n}-P_{X}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Idempotent matrix.
 
\begin_inset Formula $P_{X}P_{X}=P_{X},$
\end_inset

 
\begin_inset Formula $M_{X}M_{X}=M_{X}$
\end_inset

.
 
\begin_inset Formula $P_{X}M_{X}=M_{X}P_{X}=0$
\end_inset

.
\end_layout

\begin_layout Standard
Some properties of the residual
\end_layout

\begin_layout Itemize
\begin_inset Formula $\widehat{e}=Y-\widehat{Y}=Y-X\widehat{\beta}=M_{X}Y=M_{X}\left(X\beta+e\right)=M_{X}e$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $X'\widehat{e}=XM_{X}e=0$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}=0$
\end_inset

 if 
\begin_inset Formula $x_{i}$
\end_inset

 contains a constant.
\end_layout

\begin_layout Subsection
Goodness of Fit
\end_layout

\begin_layout Standard
The so-called R-square is the most popular measure of goodness-of-fit in
 the linear regression.
 R-square is defined when a constant is included in the regressors.
 Let 
\begin_inset Formula $M_{\iota}=I_{n}-\frac{1}{n}\iota\iota'$
\end_inset

, where 
\begin_inset Formula $\iota$
\end_inset

 is an 
\begin_inset Formula $n\times1$
\end_inset

 vector of 1's.
 
\begin_inset Formula $M_{\iota}$
\end_inset

 is the demeaner, that is, 
\begin_inset Formula $M_{\iota}\left(z_{1},\ldots,z_{n}\right)'=\left(z_{1}-\overline{z},\ldots,z_{n}-\overline{z}\right)'$
\end_inset

, where 
\begin_inset Formula $\bar{z}=\frac{1}{n}\sum_{i=1}^{n}z_{i}$
\end_inset

.
 For any 
\begin_inset Formula $X$
\end_inset

, we can decompose 
\begin_inset Formula $Y=P_{X}Y+M_{X}Y=\widehat{Y}+\widehat{e}$
\end_inset

.
 The squared sum is 
\begin_inset Formula 
\begin{eqnarray*}
Y'M_{\iota}Y & = & \left(\widehat{Y}+\widehat{e}\right)'M_{\iota}\left(\widehat{Y}+\widehat{e}\right)\\
 & = & \widehat{Y}'M_{\iota}\widehat{Y}+2\widehat{Y}'M_{\iota}\widehat{e}+\widehat{e}'M_{\iota}\widehat{e}\\
 & = & \widehat{Y}'M_{\iota}\widehat{Y}+\widehat{e}'\widehat{e}.
\end{eqnarray*}

\end_inset

where the last equality follows as 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}=0$
\end_inset

 so that 
\begin_inset Formula $M_{\iota}\widehat{e}=\widehat{e}$
\end_inset

, and 
\begin_inset Formula $\widehat{Y}'\widehat{e}=Y'P_{X}M_{X}e=0$
\end_inset

.
 R-square is defined as 
\begin_inset Formula $1-\widehat{e}'\widehat{e}/Y'M_{\iota}Y$
\end_inset

.
\end_layout

\begin_layout Subsection
Frish-Waugh-Lovell Theorem
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $y_{i}=x_{1}'\beta_{1}+x_{2}'\beta_{2}+e$
\end_inset

, then 
\begin_inset Formula $\widehat{\beta}_{1}=\left(X_{1}'M_{2}X_{1}\right)^{-1}X_{1}'M_{2}Y.$
\end_inset


\end_layout

\begin_layout Section
Statistical Properties of Least Squares
\end_layout

\begin_layout Subsection
Bias and Variance
\end_layout

\begin_layout Standard
When we discuss the statistical properties of the least squares in finite
 sample, we assume 
\begin_inset Formula 
\begin{eqnarray*}
y & = & x'\beta+e\\
E[e|x] & = & 0,
\end{eqnarray*}

\end_inset

which is equivalent to assume 
\begin_inset Formula $E\left[y|x\right]=x'\beta$
\end_inset

 so the linear projection model and the conditional mean model coincide.
 In other words, the following statistical properties hold only when the
 conditional mean is a linear function.
\end_layout

\begin_layout Itemize
Unbiasedness: 
\begin_inset Formula $E\left[\widehat{\beta}|X\right]=E\left[\left(X'X\right)^{-1}XY|X\right]=E\left[\left(X'X\right)^{-1}X\left(X'\beta+e\right)|X\right]=\beta$
\end_inset

.
\end_layout

\begin_layout Itemize
Variance 
\begin_inset Formula $\mathrm{var}\left(\widehat{\beta}|X\right)=E\left[\left(\widehat{\beta}-E\widehat{\beta}\right)\left(\widehat{\beta}-E\widehat{\beta}\right)'|X\right]=E\left[\left(X'X\right)^{-1}X'ee'X\left(X'X\right)^{-1}|X\right]=\left(X'X\right)^{-1}X'E\left[ee'|X\right]X\left(X'X\right)^{-1}.$
\end_inset

 Under the 
\emph on
homoskedasticity
\emph default
 assumption,
\begin_inset Foot
status open

\begin_layout Plain Layout
The error term is homoskedastic if 
\begin_inset Formula $E\left[e_{i}^{2}|X\right]=\sigma^{2}$
\end_inset

 for all 
\begin_inset Formula $i$
\end_inset

.
 Homoskedasticity is a restrictive assumption.
 An example of 
\emph on
heteroskedasticity
\emph default
: consumption = income + e.
 A rich family has more variation in consumption than a poor family.
\end_layout

\end_inset

 we simplify it as 
\begin_inset Formula 
\[
\mathrm{var}\left(\widehat{\beta}\right)=\left(X'X\right)^{-1}X'\left(\sigma^{2}I_{n}\right)X\left(X'X\right)^{-1}=\sigma^{2}\left(X'X\right)^{-1}.
\]

\end_inset


\end_layout

\begin_layout Subsection
Gauss-Markov Theorem
\end_layout

\begin_layout Standard
Gauss-Markov theorem justifies the OLS estimator as the efficient estimator
 among all linear unbiased ones.
 Efficient here means that it enjoys the smallest variance in a family of
 estimators.
\end_layout

\begin_layout Standard
There are numerous linearly unbiased estimators.
 For example, 
\begin_inset Formula $\left(Z'X\right)^{-1}Z'y$
\end_inset

 for 
\begin_inset Formula $z_{i}=x_{i}^{2}$
\end_inset

 is unbiased because 
\begin_inset Formula $E\left[\left(Z'X\right)^{-1}Z'y\right]=E\left[\left(Z'X\right)^{-1}Z'\left(X\beta+e\right)\right]=\beta$
\end_inset

.
 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\tilde{\beta}=A'y$
\end_inset

 be a generic linear estimator, where 
\begin_inset Formula $A$
\end_inset

 is any 
\begin_inset Formula $n\times K$
\end_inset

 functions of 
\begin_inset Formula $X$
\end_inset

.
 As 
\begin_inset Formula 
\[
E\left[A'y|X\right]=E\left[A'\left(X\beta+e\right)|X\right]=AX\beta.
\]

\end_inset

So the unbiasedness of 
\begin_inset Formula $\tilde{\beta}$
\end_inset

 implies 
\begin_inset Formula $AX=I_{n}$
\end_inset

.
 Moreover, the variance 
\begin_inset Formula 
\[
\mbox{var}\left(A'y|X\right)=E\left[\left(A'y-\beta\right)\left(A'y-\beta\right)'|X\right]=E\left[A'ee'A|X\right]=\sigma^{2}A'A.
\]

\end_inset

 Let 
\begin_inset Formula $A=C+X\left(X'X\right)^{-1}.$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
 &  & A'A-\left(X'X\right)^{-1}\\
 & = & \left(C+X\left(X'X\right)^{-1}\right)'\left(C+X\left(X'X\right)^{-1}\right)-\left(X'X\right)^{-1}\\
 & = & C'C+\left(X'X\right)^{-1}X'C+C'X\left(X'X\right)^{-1}=C'C,
\end{eqnarray*}

\end_inset

where the last equality follows as 
\begin_inset Formula 
\[
\left(X'X\right)^{-1}X'C=\left(X'X\right)^{-1}X'\left(A-X\left(X'X\right)^{-1}\right)=\left(X'X\right)^{-1}-\left(X'X\right)^{-1}=0.
\]

\end_inset

The variance of any 
\begin_inset Formula $\tilde{\beta}$
\end_inset

 is no smaller than the OLS estimator 
\begin_inset Formula $\widehat{\beta}$
\end_inset

.
 
\end_layout

\begin_layout Standard
However, notice that Gauss-Markov theorem is derived only in the conditional
 mean model under homoskedasticity.
 These conditions are restrictive.
\end_layout

\begin_layout Subsection
Variance Estimation
\end_layout

\begin_layout Standard
Under homoskedasticity, 
\begin_inset Formula $\mathrm{var}\left(\widehat{\beta}\right)=\sigma^{2}\left(X'X\right)^{-1}$
\end_inset

.
 Popular estimator of 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is the sample mean of the residuals 
\begin_inset Formula $\widehat{\sigma}^{2}=\frac{1}{n}\widehat{e}'\widehat{e}$
\end_inset

 or the unbiased one 
\begin_inset Formula $s^{2}=\frac{1}{n-K}\widehat{e}'\widehat{e}$
\end_inset

.
\end_layout

\begin_layout Standard
Under heteroskedasticity, 
\begin_inset Formula 
\[
\mathrm{var}\left(\widehat{\beta}\right)=\left(X'X\right)^{-1}X'DX\left(X'X\right)^{-1}
\]

\end_inset

where 
\begin_inset Formula $D=\mbox{diag}\left(\sigma_{1}^{2},\ldots,\sigma_{n}^{2}\right)$
\end_inset

.
 We propose to estimate 
\begin_inset Formula $\widehat{\sigma}_{i}^{2}=\widehat{e}_{i}^{2}$
\end_inset

 so that 
\begin_inset Formula $X'DX=\sum_{i=1}^{n}x_{i}x_{i}'\widehat{e}_{i}^{2}$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Normal Regression
\end_layout

\begin_layout Standard
If we assume 
\begin_inset Formula $e_{i}\sim N\left(0,\sigma^{2}\right)$
\end_inset

 is independent of 
\begin_inset Formula $x_{i}$
\end_inset

, then 
\begin_inset Formula $y_{i}\sim N\left(x_{i}'\beta,\sigma^{2}\right)$
\end_inset

.
 The likelihood of observing a sample 
\begin_inset Formula $\left(y_{i},x_{i}\right)_{i=1}^{n}$
\end_inset

 is 
\begin_inset Formula 
\[
\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2\sigma^{2}}\left(y_{i}-x_{i}'\beta\right)^{2}\right),
\]

\end_inset

 and the log-likelihood function is 
\begin_inset Formula 
\[
L\left(\beta,\sigma\right)=-\frac{n}{2}\log2\pi-n\log\sigma-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}-x_{i}'\beta\right)^{2}.
\]

\end_inset

Therefore, the maximum likelihood estimator coincides with the OLS estimator.
 
\end_layout

\begin_layout Standard
Moreover, consider the statistic
\begin_inset Formula 
\[
T_{k}=\frac{\widehat{\beta}_{k}-\beta_{k}}{\sqrt{s^{2}\left(X'X\right)_{kk}^{-1}}}=\frac{\eta_{k}'\left(X'X\right)^{-1}X'e/\sigma}{\sqrt{\frac{s^{2}}{\sigma^{2}}\eta_{k}'\left(X'X\right)^{-1}\eta_{k}}}=\frac{a_{k}'\frac{e}{\sigma}/\sqrt{a_{k}'a_{k}}}{\sqrt{s^{2}/\sigma^{2}}}
\]

\end_inset

where 
\begin_inset Formula $\eta_{k}=\left(1\left\{ l=k\right\} \right)_{l=1,\ldots,K}$
\end_inset

 is the selector, and 
\begin_inset Formula $a_{k}=\eta_{k}'\left(X'X\right)^{-1}X'$
\end_inset

.
 It is easy to see that the numerator follows a standard normal, and the
 denominator follows a 
\begin_inset Formula $\chi^{2}$
\end_inset

 with the degree of freedom 
\begin_inset Formula $n-K$
\end_inset

.
 Therefore 
\begin_inset Formula $T_{k}\sim t\left(n-K\right)$
\end_inset

.
\end_layout

\end_body
\end_document
