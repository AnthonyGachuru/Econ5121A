#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Lecture Notes, \today}
\rhead{Zhentao Shi}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
theorems-ams-extended
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing double
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard

\series bold
Notation
\series default
: in this note, 
\begin_inset Formula $y$
\end_inset

 is a scale random variable, and 
\begin_inset Formula $x$
\end_inset

 is a 
\begin_inset Formula $K\times1$
\end_inset

 random vector.
\end_layout

\begin_layout Section
Conditional Expectation Model
\begin_inset CommandInset label
LatexCommand label
name "conditional-expectation-function"

\end_inset


\end_layout

\begin_layout Standard
A regression model can be written as 
\begin_inset Formula 
\[
y=m\left(x\right)+\epsilon,
\]

\end_inset

 where 
\begin_inset Formula $m(x)=E[y|x]$
\end_inset

 is called the 
\emph on
conditional mean function
\emph default
, and 
\begin_inset Formula $\epsilon=y-m\left(x\right)$
\end_inset

 is called the 
\emph on
regression error
\emph default
.
 Such an equation holds for 
\begin_inset Formula $\left(y,x\right)$
\end_inset

 that follows any joint distribution, as long as 
\begin_inset Formula $E\left[y|x\right]$
\end_inset

 exists.
 The error term 
\begin_inset Formula $\epsilon$
\end_inset

 satisfies these properties:
\end_layout

\begin_layout Itemize
\begin_inset Formula $E\left[\epsilon|x\right]=0$
\end_inset

, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $E\left[\epsilon\right]=0$
\end_inset

, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $E\left[h\left(x\right)\epsilon\right]=0$
\end_inset

, where 
\begin_inset Formula $h$
\end_inset

 is a function of 
\begin_inset Formula $x$
\end_inset

.
 
\end_layout

\begin_layout Standard
The last property implies that 
\begin_inset Formula $\epsilon$
\end_inset

 is uncorrelated with any function of 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
If we are interested in predicting 
\begin_inset Formula $y$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

, then the conditional mean function 
\begin_inset Formula $E\left[y|x\right]$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

optimal
\begin_inset Quotes erd
\end_inset

 in terms of the 
\emph on
mean squared error
\emph default
 (MSE).
 
\end_layout

\begin_layout Standard
As 
\begin_inset Formula $y$
\end_inset

 is not a deterministic function of 
\begin_inset Formula $x$
\end_inset

, we cannot predict it with certainty.
 In order to evaluate different methods of prediction, we must therefore
 propose a criterion for comparison.
 For an arbitrary prediction method 
\begin_inset Formula $g\left(x\right)$
\end_inset

, we employ a 
\emph on
loss function
\emph default
 
\begin_inset Formula $L\left(y,g\left(x\right)\right)$
\end_inset

 to measure how wrong is the prediction, and the expected value of the loss
 function is called the 
\emph on
risk 
\begin_inset Formula $R\left(y,g\left(x\right)\right)$
\end_inset

.
 
\emph default
The 
\emph on
quadratic loss function
\emph default
 is defined as 
\begin_inset Formula 
\[
L\left(y,g\left(x\right)\right)=\left(y-g\left(x\right)\right)^{2},
\]

\end_inset

 and the corresponding risk 
\begin_inset Formula 
\[
R\left(y,g\left(x\right)\right)=E\left[\left(y-g\left(x\right)\right)^{2}\right]
\]

\end_inset

 is called the MSE.
 
\end_layout

\begin_layout Standard
Due to its operational convenience, MSE is one of the most widely used criterion.
 Under MSE, the conditional expectation function happens to be the best
 prediction method for 
\begin_inset Formula $y$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

.
 In other words, the conditional mean function 
\begin_inset Formula $m\left(x\right)$
\end_inset

 minimizes the MSE.
\end_layout

\begin_layout Standard
We can take a guess-and-verify approach to easy confirm this claim of optimality.
 For a arbitrary 
\begin_inset Formula $g\left(x\right)$
\end_inset

, the risk can be decomposed into three terms 
\begin_inset Formula 
\begin{eqnarray*}
 &  & E\left[\left(y-g\left(x\right)\right)^{2}\right]\\
 & = & E\left[\left(y-m\left(x\right)\right)^{2}\right]+2E\left[\left(y-m\left(x\right)\right)\left(m\left(x\right)-g\left(x\right)\right)\right]+E\left[\left(m\left(x\right)-g\left(x\right)\right)^{2}\right].
\end{eqnarray*}

\end_inset

The first term is irrelevant to 
\begin_inset Formula $g\left(x\right)$
\end_inset

.
 The second term 
\begin_inset Formula $2E\left[\epsilon\left(m\left(x\right)-g\left(x\right)\right)\right]=0$
\end_inset

 is again irrelevant of 
\begin_inset Formula $g\left(x\right)$
\end_inset

.
 The third term, obviously, is minimized at 
\begin_inset Formula $g\left(x\right)=m\left(x\right)$
\end_inset

.
\end_layout

\begin_layout Section
Linear Projection Model
\begin_inset CommandInset label
LatexCommand label
name "linear-projection-model"

\end_inset


\end_layout

\begin_layout Standard
As discussed in the previous section, we are interested in the conditional
 mean function 
\begin_inset Formula $m(x)$
\end_inset

.
 However, remind that 
\begin_inset Formula 
\[
m\left(x\right)=E\left[y|x\right]=\int yf\left(y|x\right)\mathrm{d}y
\]

\end_inset

 is a complex function of 
\begin_inset Formula $x$
\end_inset

, as it depends on the joint distribution of 
\begin_inset Formula $\left(y,x\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
A particular form of the conditional mean function is 
\begin_inset Formula 
\[
m\left(x\right)=x'\beta,
\]

\end_inset

a linear function of 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Remark*
The linear function is not as restrictive as one might thought.
 It can be used to generate some nonlinear (in random variables) effect
 if we re-define 
\begin_inset Formula $x$
\end_inset

.
 For example, if 
\begin_inset Formula 
\[
y=x_{1}\beta_{2}+x_{2}\beta_{2}+x_{1}x_{2}\beta_{3}+e,
\]

\end_inset

 then 
\begin_inset Formula $\frac{\partial}{\partial x_{1}}m\left(x_{1},x_{2}\right)=\beta_{1}+x_{2}\beta_{3}$
\end_inset

, which is nonlinear in 
\begin_inset Formula $x_{1}$
\end_inset

, while it is still linear in the parameter 
\begin_inset Formula $\beta$
\end_inset

 if we define a set of new regressors as 
\begin_inset Formula $\left(\tilde{x}_{1},\tilde{x}_{2},\tilde{x}_{3}\right)=\left(x_{1},x_{2},x_{1}x_{2}\right)$
\end_inset

.
 
\end_layout

\begin_layout Example*
It is true if 
\begin_inset Formula $\left(y,x\right)$
\end_inset

 follows a joint normal distribution.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
to be completed
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Even though in general 
\begin_inset Formula $m\left(x\right)\neq x'\beta$
\end_inset

, the linear form 
\begin_inset Formula $x'\beta$
\end_inset

 is still useful as an approximation, as will be clear soon.
 Therefore, we may write the linear regression model, or the 
\emph on
linear projection model
\emph default
, as 
\begin_inset Formula 
\begin{eqnarray}
y & = & x'\beta+e\label{eq:linear}\\
E[xe] & = & 0,\label{eq:Xe0}
\end{eqnarray}

\end_inset

where 
\begin_inset Formula $e$
\end_inset

 is called the 
\emph on
projection error
\emph default
, to be distinguished from 
\begin_inset Formula $\varepsilon=y-m\left(x\right)$
\end_inset

.
 
\end_layout

\begin_layout Remark*
If a constant is included in 
\begin_inset Formula $x$
\end_inset

, we have 
\begin_inset Formula $E\left[e\right]=0$
\end_inset

.
\end_layout

\begin_layout Standard
The coefficient 
\begin_inset Formula $\beta$
\end_inset

 in the linear projection model has a straightforward closed-form.
 Multiplying 
\begin_inset Formula $x$
\end_inset

 on both sides and taking expectation, we have 
\begin_inset Formula $E[xy]=E[xx']\beta$
\end_inset

.
 If 
\begin_inset Formula $E[xx']$
\end_inset

 is invertible, we can explicitly solve 
\begin_inset Formula 
\[
\beta=\left(E\left[xx'\right]\right)^{-1}E\left[xy\right].
\]

\end_inset


\end_layout

\begin_layout Standard
Now we justify 
\begin_inset Formula $x'\beta$
\end_inset

 as an approximation to 
\begin_inset Formula $m\left(x\right)$
\end_inset

.
 We claim that 
\begin_inset Formula $x'\beta$
\end_inset

 is the best as is the 
\emph on
linear
\emph default
 minimizer of the MSE; in other words, 
\begin_inset Formula 
\begin{equation}
\beta=\arg\min_{\beta\in\mathbb{R}^{K}}E\left[\left(y-x'\beta\right)^{2}\right].\label{eq:min_MSE}
\end{equation}

\end_inset

This fact can be verified by taking the first-order condition of the above
 minimization problem 
\begin_inset Formula $\frac{\partial}{\partial\beta}E\left[\left(y-x'\beta\right)^{2}\right]=2E\left[x\left(y-x'\beta\right)\right]=0.$
\end_inset


\end_layout

\begin_layout Standard
In the meantime, 
\begin_inset Formula $x'\beta$
\end_inset

 is also the best 
\emph on
linear
\emph default
 approximation to 
\begin_inset Formula $m(x)$
\end_inset

.
 If we replace 
\begin_inset Formula $y$
\end_inset

 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:min_MSE"

\end_inset

) by 
\begin_inset Formula $m\left(x\right)$
\end_inset

, we solve the minimizer as 
\begin_inset Formula 
\[
\left(E\left[xx'\right]\right)^{-1}E\left[xm\left(x\right)\right]=\left(E\left[xx'\right]\right)^{-1}E\left[E\left[xy|x\right]\right]=\left(E\left[xx'\right]\right)^{-1}E\left[xy\right]=\beta.
\]

\end_inset

Therefore 
\begin_inset Formula $\beta$
\end_inset

 is also the linear minimizer of 
\begin_inset Formula $E\left[\left(m\left(x\right)-x'\beta\right)^{2}\right]$
\end_inset

, the best linear approximation to 
\begin_inset Formula $m\left(x\right)$
\end_inset

 under MSE.
\begin_inset Note Note
status open

\begin_layout Plain Layout
revised up here 9/8/2016
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Subvector Regression
\begin_inset CommandInset label
LatexCommand label
name "subvector-regression"

\end_inset


\end_layout

\begin_layout Standard
Sometimes we are interested in a subvector of 
\begin_inset Formula $\beta$
\end_inset

.
 For example, when we include a constant and some variables in 
\begin_inset Formula $x$
\end_inset

, we are often more interested in the slope coefficients (those associated
 with the random variables), as they represent the effect of these explanatory
 factors.
 In the regression 
\begin_inset Formula 
\[
y=\beta_{1}+x'\beta_{2}+e,
\]

\end_inset

we take an expectation to get 
\begin_inset Formula $E\left[y\right]=\beta_{1}+E\left[x\right]'\beta_{2}.$
\end_inset

 Differentiate the two equations to get 
\begin_inset Formula 
\[
y-E\left[y\right]=\left(x-E\left[x\right]\right)'\beta_{2},
\]

\end_inset

so that 
\begin_inset Formula 
\[
\beta_{2}=\left(E\left[\left(x-E\left[x\right]\right)\left(x-E\left[x\right]\right)'\right]\right)^{-1}E\left[\left(x-E\left[x\right]\right)\left(y-E\left[y\right]\right)\right]=\left(\mbox{var}\left(x\right)\right)^{-1}\mbox{cov}\left(x,y\right),
\]

\end_inset

This is a special case of the subvector regression.
\end_layout

\begin_layout Standard
To discuss the general case, we need to know the formula of the inverse
 of the partitioned matrix, a fact from linear algebra.
 If 
\begin_inset Formula $Q=\begin{pmatrix}Q_{11} & Q_{12}\\
Q_{21} & Q_{22}
\end{pmatrix}$
\end_inset

 is a symmetric and positive definite matrix, then 
\begin_inset Formula 
\[
Q^{-1}=\begin{pmatrix}\left(Q_{11}-Q_{12}Q_{22}Q_{21}\right)^{-1} & -\left(Q_{11}-Q_{12}Q_{22}Q_{21}\right)^{-1}Q_{12}Q_{22}^{-1}\\
-\left(Q_{22}-Q_{21}Q_{11}Q_{12}\right)^{-1}Q_{21}Q_{11}^{-1} & \left(Q_{22}-Q_{21}Q_{11}Q_{12}\right)^{-1}
\end{pmatrix}.
\]

\end_inset

Let 
\begin_inset Formula $A_{11\cdot2}=E\left[x_{1}x_{1}'\right]-E\left[x_{1}x_{2}'\right]\left(E\left[x_{2}x_{2}'\right]\right)^{-1}E\left[x_{2}x_{1}'\right]$
\end_inset

, and 
\begin_inset Formula $A_{1y\cdot2}=E\left[x_{1}y\right]-E\left[x_{1}x_{2}'\right]\left(E\left[x_{2}x_{2}'\right]\right)^{-1}E\left[x_{2}y\right]$
\end_inset

 then 
\begin_inset Formula $\beta_{1}=A_{11\cdot2}^{-1}A_{1y\cdot2}$
\end_inset

.
 It is useful in interpreting the partial effect of a single regressors.
 We first run a regression
\begin_inset Foot
status open

\begin_layout Plain Layout
We allow 
\begin_inset Formula $x_{1}$
\end_inset

 to be a vector.
 However, one may find it is easier to consider the special case that 
\begin_inset Formula $x_{1}$
\end_inset

 is a scalar random variable.
\end_layout

\end_inset

 
\begin_inset Formula 
\begin{eqnarray*}
x_{1} & = & x_{2}'\gamma+u\\
E\left[x_{2}u\right] & = & 0
\end{eqnarray*}

\end_inset

so that 
\begin_inset Formula 
\[
u=x_{1}-x_{2}'\gamma=x_{1}-x_{2}'\left(E\left[x_{2}x_{2}'\right]\right)^{-1}E\left[x_{2}x_{1}'\right]=x_{1}-E\left[x_{1}x_{2}'\right]\left(E\left[x_{2}x_{2}'\right]\right)^{-1}x_{2}
\]

\end_inset

 We then run a simple regression of 
\begin_inset Formula $y$
\end_inset

 on 
\begin_inset Formula $u$
\end_inset

, and the coefficient is 
\begin_inset Formula 
\[
\theta=\left(E\left[uu'\right]\right)^{-1}E\left[u'y\right].
\]

\end_inset

The nominator 
\begin_inset Formula $E\left[u'y\right]=E\left[x_{1}y\right]-E\left[x_{1}x_{2}'\right]\left(E\left[x_{2}x_{2}'\right]\right)^{-1}E\left[x_{2}y\right]$
\end_inset

.
 The denominator 
\begin_inset Formula 
\begin{eqnarray*}
E\left[uu'\right] & = & E\left[\left(x_{1}-E\left[x_{1}x_{2}'\right]\left(E\left[x_{2}x_{2}'\right]\right)^{-1}x_{2}\right)\left(x_{1}-E\left[x_{1}x_{2}'\right]\left(E\left[x_{2}x_{2}'\right]\right)^{-1}x_{2}\right)'\right]=A_{11\cdot2}.
\end{eqnarray*}

\end_inset

We have verified that 
\begin_inset Formula $\beta_{1}=\theta$
\end_inset

.
\end_layout

\begin_layout Subsection
Omitted Variable Bias
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "omitted-variable-bias"

\end_inset


\end_layout

\begin_layout Standard
Long regression is 
\begin_inset Formula $y=x_{1}'\beta_{1}+x_{2}'\beta_{2}+e$
\end_inset

, and short regression is 
\begin_inset Formula $y=x_{1}'\gamma+u.$
\end_inset

 To discuss how to sign the bias, we first demean all the variables, which
 is equivalent as if we project out the effect of the constant.
 The long regression becomes 
\begin_inset Formula 
\[
\begin{aligned}\tilde{y} & = & \tilde{x}_{1}'\beta_{1}+\tilde{x}_{2}'\beta_{2}+e\end{aligned}
,
\]

\end_inset

 and the short regression becomes 
\begin_inset Formula 
\[
\tilde{y}=\tilde{x}_{1}'\gamma+u,
\]

\end_inset

 where 
\emph on
tilde
\emph default
 denotes the demeaned variable.
 
\end_layout

\begin_layout Standard
After demeaning, the cross moment equals to the covariance.
 The short regression coefficient 
\begin_inset Formula 
\begin{eqnarray*}
\gamma & = & \left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{y}\right]\\
 & = & \left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\left(\tilde{x}_{1}'\beta_{1}+\tilde{x}_{2}'\beta_{2}+e\right)\right]\\
 & = & \beta_{1}+\left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{x}_{2}'\right]\beta_{2}.
\end{eqnarray*}

\end_inset

Therefore, 
\begin_inset Formula $\gamma=\beta_{1}$
\end_inset

 if and only if 
\begin_inset Formula $E\left[\tilde{x}_{1}\tilde{x}_{2}'\right]\beta_{2}=0$
\end_inset

, which means either 
\begin_inset Formula $E\left[\tilde{x}_{1}\tilde{x}_{2}'\right]=0$
\end_inset

 or 
\begin_inset Formula $\beta_{2}=0$
\end_inset

.
\end_layout

\end_body
\end_document
